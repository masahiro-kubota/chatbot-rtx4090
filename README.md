It runs on a GPU with 24GB of VRAM (e.g., RTX 4090).
It's using Qwen2-VL-7B. If thereâ€™s a better VLM, please let me know!
```
# to start chatbot
./start-ollama.sh
```

```
# to stop docker containers
docker compose down
```

Requirements
- NVIDIA Container Toolkit
- Docker
